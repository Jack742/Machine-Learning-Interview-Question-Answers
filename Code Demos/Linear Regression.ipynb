{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Valentina Alto has a great blog post on OLS linear regression! read it here: https://towardsdatascience.com/understanding-the-ols-method-for-simple-linear-regression-e0a4e8f692cc*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares Linear Regression\n",
    "Linear regression is a simple linear approach that models the relationship between input variables $X$ and the single output variable $Y$\n",
    "\n",
    "This can be implemented in a number of ways, most commonly through either ordinary least squares (OLS).\n",
    "\n",
    "The linear regression problem can be formulated as such:\n",
    "\n",
    "$Y = \\alpha + \\Sigma_{i=1..p} \\beta_{i}X_{i} + \\epsilon_{i}$\n",
    "\n",
    "where $\\epsilon$ is the error term, $\\alpha$ is the value of the dependent variable $Y$ when the independent variables are $0$, and $\\beta_{j}$ is the weight applied to the independent variable $X_{j}$\n",
    "\n",
    "For now, lets consider the simple case where there is only one independent variable:\n",
    "\n",
    "$Y = \\alpha + \\beta X + \\epsilon$\n",
    "\n",
    "or, for a single data point:\n",
    "\n",
    "$y_{i} = \\alpha + \\beta x_{i} + \\epsilon_{i}$\n",
    "\n",
    "The goal here is to select values for $\\alpha$ and $\\beta$ that minimise the error term $\\epsilon$. We can start by rearranging the formula:\n",
    "\n",
    "$\\epsilon_{i} = y_{i} - \\alpha - \\beta x_{i}$\n",
    "\n",
    "thus for all data points...\n",
    "\n",
    "$\\Sigma_{i=1}^{n}\\epsilon_{i} = \\Sigma_{i=1}^{n}(y_{i} - \\alpha - \\beta x_{i}$)\n",
    "\n",
    "As error could be positive or negative, we square $\\epsilon$ to ensure all error contributes positively to the term:\n",
    "\n",
    "$\\Sigma_{i=1}^{n}\\epsilon_{i}^{2} = \\Sigma_{i=1}^{n}(y_{i} - \\alpha - \\beta x_{i})^{2}$\n",
    "\n",
    "Now we can set about minimising $\\epsilon$ by choosing good values for $\\alpha$ and $\\beta$. This is achieved through calculating the partial derivative of the function with respect to the parameters\n",
    "\n",
    "let $L(\\alpha, \\beta) = \\Sigma_{i=1}^{n}\\epsilon_{i}^{2} = \\Sigma_{i=1}^{n}(y_{i} - \\alpha - \\beta x_{i})^{2}$\n",
    "\n",
    "\n",
    "\n",
    "Set to $0$ as this is the point of inflection (i.e. the minima)\n",
    "\n",
    "\n",
    "$L(\\alpha \\beta) = \\Sigma_{i=1}^{n}(y_{i} - \\alpha - \\beta x_{i})^{2}$\n",
    "\n",
    "$L(\\alpha \\beta) = \\Sigma_{i=1}^{n}(y_{i} - \\alpha - \\beta x_{i})(y_{i} - \\alpha - \\beta x_{i})$\n",
    "\n",
    "$L(\\alpha \\beta) = \\Sigma_{i=1}^{n} y_{i}^{2} + \\alpha^{2} + (\\beta x_{i})^{2} - 2y_{i}\\alpha - 2y_{i}\\beta x_{i} + 2abx_{i}$\n",
    "\n",
    "\n",
    "Solve for $\\beta$: \n",
    "\n",
    "$\\frac{\\delta L(\\alpha, \\beta)}{\\delta \\beta} = 0$\n",
    "\n",
    "$\\Sigma_{i=1}^{n} 2\\alpha x_{i} - 2y_{i}x_{i} + 2\\beta x_{i} = 0$ \n",
    "\n",
    "$\\Sigma_{i=1}^{n}(y_{i} - \\alpha - \\beta x_{i})x_{i} = 0$\n",
    "\n",
    "substituting $\\Sigma_{i=1}^{n}\\alpha = \\Sigma_{i=1}^{n}y_{i}-\\beta x_{i}$\n",
    "$\\alpha = \\frac{\\Sigma_{i=1}^{n}y_{i}-\\beta x_{i}}{\\Sigma_{i=1}^{n}}$\n",
    "\n",
    "$\\therefore \\alpha = \\bar{y} - \\beta \\bar{x}$\n",
    "\n",
    "$\\Sigma_{i=1}^{n}(y_{i} - (\\bar{y} - \\beta \\bar{x}) - \\beta x_{i})x_{i} = 0$\n",
    "\n",
    "$\\Sigma_{i=1}^{n} y_{i} - \\bar{y} + \\beta \\bar{x} - \\beta x_{i} = 0$\n",
    "\n",
    "$\\beta \\Sigma_{i=1}^{n}(\\bar{x} - \\beta x_{i}) = \\Sigma_{i=1}^{n}(\\bar{y} - y_{i})$\n",
    "\n",
    "$\\beta = \\frac{\\Sigma_{i=1}^{n}(\\bar{y} - y_{i})}{\\Sigma_{i=1}^{n}(\\bar{x} - \\beta x_{i})}$\n",
    "\n",
    "\n",
    "Solve for $\\alpha$:\n",
    "\n",
    "$\\frac{\\delta L(\\alpha, \\beta)}{\\delta \\alpha} = 0$\n",
    "\n",
    "$\\Sigma_{i=1}^{n} 2\\alpha - 2y_{i} + 2\\beta x_{i} = 0$ \n",
    "\n",
    "$\\Sigma_{i=1}^{n} y_{i} -\\alpha - \\beta x_{i} = 0$ \n",
    "\n",
    "$\\Sigma_{i=1}^{n} \\alpha = \\Sigma_{i=1}^{n} y_{i} - \\beta x_{i}$ \n",
    "\n",
    "$\\alpha = \\frac{\\Sigma_{i=1}^{n} y_{i} - \\beta x_{i}}{\\Sigma_{i=1}^{n}}$\n",
    "\n",
    "$\\alpha = \\bar{y} - \\beta \\bar{x}$\n",
    "\n",
    "with $\\alpha$ and $\\beta$ found, we now have our regression model by substituting these values back in to the equation above:\n",
    "\n",
    "$y = \\alpha + \\beta x$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get dataset\n",
    "Split into test and train\n",
    "\"\"\"\n",
    "\n",
    "diabetes_X, diabetes_Y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "diabetes_X = diabetes_X[:,None,2]\n",
    "\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "diabetes_Y_train = diabetes_Y[:-20]\n",
    "diabetes_Y_test = diabetes_Y[-20:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59ac764984914fe8e0c657f22a743f4cd4d844eddc087eb14206854eff6f21cf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
